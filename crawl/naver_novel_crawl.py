import asyncio
from playwright.async_api import async_playwright
import pandas as pd
import random
from itertools import chain
import os
from fake_useragent import UserAgent
from tqdm import tqdm
from tqdm.asyncio import tqdm_asyncio
from datetime import datetime
import pickle

# ë°ì´í„° ë‚˜ëˆ„ê¸°
def split_data(data, split_num):
    """ë¦¬ìŠ¤íŠ¸ì™€ ë”•ì…”ë„ˆë¦¬ ëª¨ë‘ ì²˜ë¦¬í•˜ëŠ” ë²”ìš© ë¶„í•  í•¨ìˆ˜"""
    
    if isinstance(data, list):
        # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ê¸°ì¡´ ë¡œì§)
        new_data = []
        for i in range(0, len(data), split_num):
            new_data.append(data[i: i+split_num])
        return new_data
    
    elif isinstance(data, dict):
        # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
        items = list(data.items())  # (key, value) íŠœí”Œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        new_data = []
        
        for i in range(0, len(items), split_num):
            batch_items = items[i: i+split_num]
            batch_dict = dict(batch_items)  # ë‹¤ì‹œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            new_data.append(batch_dict)
        
        return new_data
    
    else:
        raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…ì…ë‹ˆë‹¤: {type(data)}")

# ë§í¬ ê°€ì ¸ì˜¤ê¸°
async def get_links(playwright, url):
    browser = await playwright.chromium.launch(headless=True)  # headlessë¡œ ë³€ê²½
    context = await browser.new_context()
    page = await context.new_page()
    
    try:
        await page.goto(url, timeout=30000)

        # ë‹¤ì–‘í•œ url íŒ¨í„´
        xpath_patterns = [
            '//*[@id="content"]/div/ul/li/div/h3'
        ]

        # ë§í¬ ê°€ì ¸ì˜¤ê¸°
        links = []

        page_list = await page.locator('xpath=//*[@id="content"]/div/ul/li/div/h3').all()
        for page_tag in page_list:
            page_title = await page_tag.inner_text()
            link = 'https://series.naver.com' + await page_tag.locator('xpath=//a[contains(@href, "/novel/detail")]').get_attribute('href')
            if '19ê¸ˆ' in page_title:    
                # links[(page_title, '19ê¸ˆ')] = link
                links.append({
                    'age': '19',
                    'url': link
                })
            else:
                links.append({
                    'age': 'ì „ì²´',
                    'url': link
                })
        
        await asyncio.sleep(random.randint(1, 2))
        return links
    
    except Exception as e:
        print(f"ë§í¬ ìˆ˜ì§‘ ì—ëŸ¬ {url}: {e}")
        return []
    finally:
        await browser.close()

# íƒœê·¸ í™•ì¸
async def extrac_xpath(page, xpaths, type='text'):
    for xpath in xpaths:
        try:
            # ë¹ˆ XPath ì²´í¬
            if not xpath or xpath.strip() == '' or xpath == 'xpath=':
                continue
                
            element = page.locator(xpath)
            if await element.count() > 0:
                if type == 'text':
                    data = await element.first.inner_text()
                elif type == 'src':
                    data = await element.first.get_attribute('src')
                
                if data and data.strip():
                    return data.strip()
        except Exception as e:
            continue
    return ''

# ì†Œì„¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
async def create_page(playwright, user_agent):
    browser = None
    try:
        browser = await playwright.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--no-first-run',
                '--disable-extensions',
                '--disable-default-apps',
                '--disable-gpu'
            ]
        )
        context = await browser.new_context(
            user_agent=user_agent,
            is_mobile=False,
            has_touch=False,
            viewport={'width': 1920, 'height': 1080}
        )
        page = await context.new_page()
        
        return page, browser
        
    except Exception as e:
        print(f'ì—ëŸ¬: {e}')

async def get_data(playwright=None, page=None, url=None, user_agent=None, age='all'):
    if age == 'all':
        page, browser = await create_page(playwright, user_agent)

    response = await page.goto(url, timeout=30000)


    if response.status >= 400:
        return {'url': url, 'status': f'HTTP_{response.status}', 'error': 'HTTP Error'}
    
    # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
    await asyncio.sleep(2)
    
    # ì´ë¯¸ì§€
    img_xpaths = [
        'xpath=/html/body/div[1]/div[2]/div[1]/span/img',
        'xpath=//*[@id="container"]/div[1]/a/img',
        'xpath=//*[@id="container"]/div[1]/span/img',
        'xpath=//*[@id="ct"]/div[1]/div[1]/div[1]/div[1]/a/img'
    ]
    img = await extrac_xpath(page, img_xpaths, type='src')

    # ì œëª©
    title_xpaths = [
        'xpath=//*[@id="content"]/div[1]/h2',
        'xpath=//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/strong'
    ]
    title = await extrac_xpath(page, title_xpaths)        

    # í‰ì 
    rating_xpaths = [
        'xpath=//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/div[1]/ul/li/span/span',
        'xpath=//*[@id="content"]/div[1]/div[1]/em'
    ]
    rating = await extrac_xpath(page, rating_xpaths)

    # ì¥ë¥´
    genre_xpaths = [
        'xpath=//*[@id="content"]/ul[1]/li/ul/li[2]/span/a',
        'xpath=//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/div[2]/ul/li[1]/dl/dd[2]'
    ]
    genre = await extrac_xpath(page, genre_xpaths)

    # ì—°ì¬ìƒíƒœ
    serial_xpaths = [
        'xpath=//*[@id="content"]/ul[1]/li/ul/li[1]/span',
        'xpath=//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/div[2]/ul/li[1]/dl/dd[1]'
    ]
    serial = await extrac_xpath(page, serial_xpaths)

    # ì¶œíŒì‚¬
    publisher_xpaths = [
        'xpath=//*[@id="content"]/ul[1]/li/ul/li[4]/a'
    ]
    publisher = await extrac_xpath(page, publisher_xpaths)

    # ì†Œì„¤ ì„¤ëª…ê¸€
    summary_xpaths = [
        'xpath=//*[@id="content"]/div[2]'
    ]

    if await page.locator('xpath=//*[@id="content"]/div[2]/div[1]/span/a').count() > 0:
        await page.locator('xpath=//*[@id="content"]/div[2]/div[1]/span/a').click()

    summary = await extrac_xpath(page, summary_xpaths)

    # ì´ í™”ìˆ˜
    page_count_xpaths = [
        'xpath=//*[@id="content"]/h5/strong'
    ]
    page_count = await extrac_xpath(page, page_count_xpaths)

    # ë‹¨ìœ„(í™”, ê¶Œ)
    page_unit_xpaths = [
        'xpath=//*[@id="content"]/h5'
    ]
    page_unit = await extrac_xpath(page, page_unit_xpaths)
    page_unit = page_unit.strip()[-1]

    age_xpaths = [
        'xpath=//*[@id="content"]/ul[1]/li/ul/li[5]'
    ]
    age = await extrac_xpath(page, age_xpaths)

    author = await extrac_xpath(page, 'xpath=//*[@id="content"]/ul[1]/li/ul/li[3]/a')
    
    novel_data = {
        'url': url,
        'img': img,
        'title': title,
        'athor': author,
        'rating': rating,
        'genre': genre,
        'serial': serial,
        'publisher': publisher,
        'summary': summary,
        'page_count': page_count,
        'page_unit': page_unit,
        'age': age,
        'platform': 'naver'
    }


    # ë””ë²„ê¹… ì •ë³´
    if not all([title, rating, genre, serial, publisher, summary, page_count, page_unit, age]):
        print(f"ë°ì´í„° ëˆ„ë½: {page.url}")
        data_fields = ['img', 'title', 'rating', 'genre', 'serial', 'publisher', 'summary', 'page_count', 'page_unit', 'age']
        filled_fields = sum(1 for field in data_fields if novel_data.get(field) and novel_data[field] != '')
        print(f"ìˆ˜ì§‘ëœ í•„ë“œ: {filled_fields}/{len(data_fields)}ê°œ")
    
    await asyncio.sleep(random.uniform(1, 2))

    if browser:
        try:
            await browser.close()
        except:
            pass

    return novel_data

# íŒŒì¼ ì €ì¥í•˜ê¸°
async def save_data(results):
    """ê²°ê³¼ ì €ì¥ í•¨ìˆ˜"""
    if not results:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('data', exist_ok=True)
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    df = pd.DataFrame(results)
    filename = f'data/naver_novel_data.csv'
    df.to_csv(filename, encoding='utf-8', index=False)
    
    
    print(f'ğŸ“ ì „ì²´ ë°ì´í„° ì €ì¥: {filename} ({len(results)}ê°œ)')

# ë¡œê·¸ì¸í•˜ê¸°
async def login(page):
    """ë¡œê·¸ì¸ ì²˜ë¦¬"""
    await page.goto('https://nid.naver.com/nidlogin.login')
    
    print("ë¡œê·¸ì¸ì„ ì™„ë£Œí•œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
    input()  # ì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ ë¡œê·¸ì¸ ì™„ë£Œí•  ë•Œê¹Œì§€ ëŒ€ê¸°


async def main():
    print("ğŸš€ ë„¤ì´ë²„ ì†Œì„¤ í¬ë¡¤ë§ ì‹œì‘!")
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('data', exist_ok=True)
    
    # ì´ë¯¸ ì¤€ë¹„ë˜ì–´ìˆëŠ” urlì´ ìˆëŠ”ì§€ í™•ì¸
    if os.path.exists('data/naver_novel_page_link_data.data'):
        print("ğŸ“‚ ê¸°ì¡´ ë§í¬ íŒŒì¼ ë¡œë“œ ì¤‘...")
        with open('data/naver_novel_page_link_data.data', 'rb') as f:
            all_urls = pickle.load(f)
        
        all_urls = list(chain.from_iterable(all_urls))

    else:
        print("ğŸ” ë§í¬ ìˆ˜ì§‘ ì‹œì‘...")
        page_urls = []
        for i in range(1, 4232):  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 5í˜ì´ì§€ë§Œ
            page_urls.append(f'https://series.naver.com/novel/categoryProductList.series?categoryTypeCode=all&page={i}')
        

        # page_urls = page_urls[:20]
        # print(len(page_urls))
        urls = split_data(page_urls, 5)

        ua = UserAgent(platforms='desktop')
        print(f"ğŸ“„ {len(page_urls)}ê°œ í˜ì´ì§€ì—ì„œ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        all_links = []
        async with async_playwright() as playwright:
            # ë§í¬ ìˆ˜ì§‘ ì§„í–‰ìƒí™© í‘œì‹œ
            for url_list in tqdm(urls, desc="ğŸ“„ í˜ì´ì§€ë³„ ë§í¬ ìˆ˜ì§‘", unit="ë°°ì¹˜"):
                tasks = [get_links(playwright, url) for url in url_list]
                
                # tqdm_asyncioë¡œ ê° ë°°ì¹˜ ë‚´ íƒœìŠ¤í¬ ì§„í–‰ìƒí™© í‘œì‹œ
                batch_results = await tqdm_asyncio.gather(
                    *tasks, 
                    desc=f"URL ì²˜ë¦¬ ({len(url_list)}ê°œ)", 
                    unit="í˜ì´ì§€",
                    # return_exceptions=True
                )
                all_links.append(batch_results)

        # í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        all_urls = list(chain.from_iterable(all_links))
        print(all_urls)
        # ì¤‘ë³µ ì œê±° ë° ì €ì¥
        # unique_links = list(set(all_links))
        print(f"ğŸ”— ìˆ˜ì§‘ëœ ê³ ìœ  ë§í¬: {len(all_urls)}ê°œ")

        
        with open('data/naver_novel_page_link_data.data', 'wb') as f:
            pickle.dump(all_urls, f)

    not_nineteen_links = [link['url'] for link in all_urls if link['age'] != 19]
    nineteen_links = [link['url'] for link in all_urls if link['age'] != 19]

    # ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘
    total_urls = sum(len(url_list) for url_list in all_urls)
    print(f"\nğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ - ì´ {total_urls}ê°œ URL")
    
    all_results = []
    # print(urls[0])
    
    ua = UserAgent(platforms='desktop')
    
    not_nineteen_links = split_data(not_nineteen_links, 5)
    # ì „ì²´ ì´ìš©ê°€ ì†Œì„¤
    async with async_playwright() as playwright:
        # ë§í¬ ìˆ˜ì§‘ ì§„í–‰ìƒí™© í‘œì‹œ
        for url_list in tqdm(not_nineteen_links, desc="ğŸ“„ í˜ì´ì§€ë³„ ë§í¬ ìˆ˜ì§‘", unit="ë°°ì¹˜"):
            tasks = [get_data(playwright=playwright, url=url, user_agent=ua.random) for url in url_list]
            
            # tqdm_asyncioë¡œ ê° ë°°ì¹˜ ë‚´ íƒœìŠ¤í¬ ì§„í–‰ìƒí™© í‘œì‹œ
            batch_results = await tqdm_asyncio.gather(
                *tasks, 
                desc=f"URL ì²˜ë¦¬ ({len(url_list)}ê°œ)", 
                unit="í˜ì´ì§€",
                # return_exceptions=True
            )
            all_results.append(batch_results)
    
    nineteen_links = split_data(nineteen_links, 5)
    # 19ì„¸ ì´ìš©ê°€ ì†Œì„¤
    async with async_playwright() as playwright:
        # ë‹¨ì¼ ë¸Œë¼ìš°ì € ë° í˜ì´ì§€ ìƒì„±
        browser = await playwright.chromium.launch(
            headless=False,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--no-first-run',
                '--disable-extensions',
                '--disable-default-apps',
                '--disable-gpu'
            ]
        )
        
        context = await browser.new_context(
            user_agent=ua.random,
            is_mobile=False,
            has_touch=False,
            viewport={'width': 1920, 'height': 1080}
        )
        
        page = await context.new_page()

        try:
            # ë¡œê·¸ì¸ ì²˜ë¦¬
            await login(page)

            # ê° í˜ì´ì§€ì—ì„œ ìˆœì°¨ì ìœ¼ë¡œ ë§í¬ ìˆ˜ì§‘
            all_links = []
            print(len(nineteen_links))
            for url in tqdm(nineteen_links, desc="ğŸ“„ í˜ì´ì§€ë³„ ë§í¬ ìˆ˜ì§‘"):
                links = await get_links(playwright=None, page=page, url=url)
                all_links.extend(links)
                print(f"ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: {len(links)}")
            
        except Exception as e:
            print(f"âŒ ë©”ì¸ ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            await browser.close()
    
    all_results = list(chain.from_iterable(all_links))
    
    # ë°ì´í„° ì €ì¥
    print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...")
    await save_data(all_results)
    
    return all_results

if __name__ == "__main__":
    try:
        results = asyncio.run(main())
        print("\nğŸŠ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()