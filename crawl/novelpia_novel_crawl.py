import asyncio
from playwright.async_api import async_playwright
import pandas as pd
import random
import os
from fake_useragent import UserAgent
from tqdm import tqdm
import pickle
from tqdm.asyncio import tqdm_asyncio
from fake_useragent import UserAgent

# ë°ì´í„° ë‚˜ëˆ„ê¸°
def split_data(data, split_num):
    """ë¦¬ìŠ¤íŠ¸ì™€ ë”•ì…”ë„ˆë¦¬ ëª¨ë‘ ì²˜ë¦¬í•˜ëŠ” ë²”ìš© ë¶„í•  í•¨ìˆ˜"""
    
    if isinstance(data, list):
        # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ê¸°ì¡´ ë¡œì§)
        new_data = []
        for i in range(0, len(data), split_num):
            new_data.append(data[i: i+split_num])
        return new_data
    
    elif isinstance(data, dict):
        # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
        items = list(data.items())  # (key, value) íŠœí”Œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        new_data = []
        
        for i in range(0, len(items), split_num):
            batch_items = items[i: i+split_num]
            batch_dict = dict(batch_items)  # ë‹¤ì‹œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            new_data.append(batch_dict)
        
        return new_data
    
    else:
        raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…ì…ë‹ˆë‹¤: {type(data)}")

# ìµœëŒ€ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
async def get_last_page(page, url):
    """ê°€ì¥ ê¸°ë³¸ì ì¸ ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§ - í˜ì´ì§€ ì¬ì‚¬ìš©"""
    try:
        # í˜ì´ì§€ ë°©ë¬¸
        await page.goto(url)
        
        await page.wait_for_selector('xpath=/html/body/div[8]/div[3]/div[7]/nav/ul/li[9]/a')
        # í´ë¦­ ëŒ€ê¸° ë° ì‹¤í–‰
        await page.locator('xpath=/html/body/div[8]/div[3]/div[7]/nav/ul/li[9]/a').click()

        # í˜ì´ì§€ ë²ˆí˜¸ ìš”ì†Œë“¤ ê°€ì ¸ì˜¤ê¸°
        await page.wait_for_selector('xpath=/html/body/div[8]/div[3]/div[7]/nav/ul/li')
        page_num = await page.locator('xpath=/html/body/div[8]/div[3]/div[7]/nav/ul/li').all()
        temp = []
        for n in page_num:
            text = await n.inner_text()
            temp.append(text)
            temp.append(0)

        temp = [int(i) for i in temp if i]
        max_num = max(temp)
        return max_num
        
    except Exception as e:
        print(f"ìµœëŒ€ í˜ì´ì§€ ìˆ˜ì§‘ ì—ëŸ¬: {e}")
        return 1

# ë§í¬ ê°€ì ¸ì˜¤ê¸°
async def get_links(page, url):
    """ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ë§í¬ ìˆ˜ì§‘"""
    try:
        await page.goto(url)
        await page.wait_for_load_state('networkidle')

        links = []
        elements = await page.locator('xpath=/html/body/div[8]/div[3]/div[6]/div/table/tbody/tr[1]/td[1]').all()
        
        for element in elements:
            onclick_value = await element.get_attribute('onclick')
            # print(onclick_value)
            if onclick_value:
                # onclick="location='/novel/25974';" ì—ì„œ URL ë¶€ë¶„ ì¶”ì¶œ
                if "location='" in onclick_value:
                    url_part = onclick_value.split("location='")[1].split("'")[0]
                    full_url = f"https://novelpia.com{url_part}"
                    links.append(full_url)
                    
                    # ì œëª©ë„ í•¨ê»˜ ì¶”ì¶œ
                    title = await element.inner_text()
                    # print(f"ì œëª©: {title}, URL: {full_url}")
        
        await asyncio.sleep(random.uniform(4, 15))
        return links
        
    except Exception as e:
        print(f"ë§í¬ ìˆ˜ì§‘ ì—ëŸ¬ {url}: {e}")
        return []

async def login(page):
    """ë¡œê·¸ì¸ ì²˜ë¦¬"""
    await page.goto('https://nid.naver.com/oauth2.0/authorize?response_type=code&client_id=A8OQVi3byB1jFOckQ0RZ&redirect_uri=https%3A%2F%2Fnovelpia.com%2Fproc%2Flogin_naver%3Fredirectrurl%3D&state=e52d643d7eca539840bb97c0f697b16c')
    
    print("ë¡œê·¸ì¸ì„ ì™„ë£Œí•œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
    input()  # ì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ ë¡œê·¸ì¸ ì™„ë£Œí•  ë•Œê¹Œì§€ ëŒ€ê¸°

async def save_data(results):
    """ê²°ê³¼ ì €ì¥ í•¨ìˆ˜"""
    if not results:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('data', exist_ok=True)
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    df = pd.DataFrame(results)
    filename = f'data/novelpia_novel_data.csv'
    df.to_csv(filename, encoding='utf-8', index=False)
    
    print(f'ğŸ“ ì „ì²´ ë°ì´í„° ì €ì¥: {filename} ({len(results)}ê°œ)')

async def extract_element(page, xpaths, type='text'):
    # ì¶”ì²œ ìˆ˜
    for xpath in xpaths:
        if await page.locator(xpath).count() > 0:
            if type == 'img':
                img = await page.locator(xpath).get_attribute('src')
                return img
            data = await page.locator(xpath).inner_text()
            return data

async def get_data(playwright, url, user_agent):
    """ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ì†Œì„¤ ë°ì´í„° ìˆ˜ì§‘"""
    browser = None
    try:
        browser = await playwright.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--no-first-run',
                '--disable-extensions',
                '--disable-default-apps',
                '--disable-gpu'
            ]
        )
        context = await browser.new_context(
            user_agent=user_agent,
            is_mobile=False,
            has_touch=False,
            viewport={'width': 1920, 'height': 1080}
        )
        page = await context.new_page()

        while True:
            response = await page.goto(url)
            if response.status != 200:
                print('í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨')
                os.system('nordvpn connect South_Korea')
                await asyncio.sleep(10)
                response = await page.goto(url, timeout=30000)
            else:
                break
        # await page.wait_for_load_state('networkidle')
        
        # ìš”ì†Œë“¤ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        # await page.wait_for_selector('.epnew-novel-title', timeout=15000)
        
        img_xpaths = [
            'xpath=/html/body/div[6]/div[1]/div[1]/a/img',
            'xpath=/html/body/div[6]/div[1]/div[1]/img',
            '.conver_img',
            'body > div:nth-child(18) > div.epnew-wrapper.s_inv.side_padding > div.epnew-cover-box > a > img'
            # 'img.s_inv:nth-child(5)'
        ]   
        img = await extract_element(page, img_xpaths, type='img')
        title_xpaths = [
            'div.epnew-novel-title',
            'xpath=/html/body/div[6]/div[1]/div[2]/div[2]'
        ]
        # title = await page.locator('.ep-info-line.epnew-novel-title').inner_text()
        title = await extract_element(page, title_xpaths)
        
        author_xpaths = [
            'xpath=/html/body/div[6]/div[1]/div[2]/div[3]/p[1]/a',
            'div.writer-name'
        ]
        author = await extract_element(page, author_xpaths)
        # author = await page.locator('.writer-name').inner_text()
        
        recommend_xpaths = [
            'xpath=/html/body/div[6]/div[1]/div[2]/div[4]/div[1]/p[2]/span[2]',
            'xpath=/html/body/div[6]/div[1]/div[2]/div[5]/div[1]/p[2]/span[2]'
        ]
        
        # ì¶”ì²œ ìˆ˜
        recommend = await extract_element(page, recommend_xpaths)
        # recommend = await page.locator('.counter-line-a p:last-child span:last-child').inner_text()
        keywords_xpaths = [
            'xpath=/html/body/div[6]/div[1]/div[2]/div[6]/div[1]/p[1]',
            'xpath=/html/body/div[6]/div[1]/div[2]/div[5]/div[1]/p[1]',
            '.writer-tag:nth-child(2)'
        ]
        keywords = await extract_element(page, keywords_xpaths)
        # keywords = await page.locator('p.writer-tag').first.inner_text()
        keywords_items = {
            'ë¡œë§¨ìŠ¤': 'ë¡œë§¨ìŠ¤',
            'ë¬´í˜‘': 'ë¬´í˜‘',
            'ë¼ì´íŠ¸ë…¸ë²¨':'ë¼ì´íŠ¸ë…¸ë²¨',
            'ê³µí¬':'ê³µí¬',
            'SF':'SF',
            'ìŠ¤í¬ì¸ ':'ìŠ¤í¬ì¸ ',
            'ëŒ€ì²´ì—­ì‚¬':'ëŒ€ì²´ì—­ì‚¬',
            'í˜„ëŒ€íŒíƒ€ì§€':'í˜„ëŒ€íŒíƒ€ì§€',
            'í˜„ëŒ€':'í˜„ëŒ€',
            'íŒíƒ€ì§€':'íŒíƒ€ì§€'
        }
        genre = None
        for key, value in keywords_items.items():
            if key in keywords:
                genre = value
                break
        
        serial_element = await page.locator('.in-badge').inner_text()
        # serial_element = await page.locator('xpath=/html/body/div[6]/div[1]/div[2]/div[3]/p[2]').inner_text()
        if 'ì™„ê²°' in serial_element:
            serial = 'ì™„ê²°'
        else:
            serial = 'ì—°ì¬ì¤‘'
        
        publisher = ''
        page_count_xpaths = [
            'xpath=/html/body/div[6]/div[1]/div[2]/div[6]/div[2]/div[1]/p[3]/span[2]',
            'xpath=/html/body/div[6]/div[1]/div[2]/div[5]/div[2]/div[1]/p[3]/span[2]',
        ]
        # page_count = await extract_element(page, page_count_xpaths)
        page_count = await page.locator('.writer-name').last.inner_text()
        page_unit = 'í™”'
        
        if '19' in serial_element:
            age = '19'
        else:
            age = 'ì „ì²´'

        viewers_xpaths = [
            'xpath=/html/body/div[6]/div[1]/div[2]/div[4]/div[1]/p[1]/span[2]',
            'xpath=/html/body/div[6]/div[1]/div[2]/div[5]/div[1]/p[1]/span[2]'
        ]
        # viewers = await extract_element(page, viewers_xpaths)
        viewers = await page.locator('.counter-line-a p:first-child span:last-child').inner_text()
        summary_xpaths = [
            'xpath=/html/body/div[6]/div[1]/div[2]/div[5]/div[2]/div[2]',
            'xpath=/html/body/div[6]/div[1]/div[2]/div[6]/div[2]/div[2]'
        ]
        # summary = await extract_element(page, summary_xpaths)
        summary = await page.locator('.synopsis').first.inner_text()
        
        novel_data = {
            'url': url,
            'img': img,
            'title': title,
            'author': author,
            'recommend': recommend,
            'genre': genre,
            'serial': serial,
            'publisher': publisher,
            'summary': summary,
            'page_count': page_count,
            'page_unit': page_unit,
            'age': age,
            'platform': 'novelpia',
            'keywords': keywords,
            'viewers': viewers
        }
        
        if any(value is None for value in novel_data.values()):
            print(novel_data)
        
        # ëœë¤ ëŒ€ê¸° ì‹œê°„
        await asyncio.sleep(random.uniform(10,30))
        return novel_data
        
    except Exception as e:
        print(f"[ERROR] {url}: {e}")
        page_source = await page.content()
        with open('page.html', 'w') as f:
            f.write(page_source)
        return None

async def main():
    print("ğŸš€ ë…¸ë²¨í”¼ì•„ ì†Œì„¤ í¬ë¡¤ë§ ì‹œì‘!")
    
    # User Agent ì„¤ì •
    ua = UserAgent()
    user_agent = ua.random
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('data', exist_ok=True)
    if os.path.exists('data/novelpia_novel_page_link_data.data'):
        with open('data/novelpia_novel_page_link_data.data', 'rb') as f:
            all_links = pickle.load(f)
    else:
        async with async_playwright() as playwright:
            # ë‹¨ì¼ ë¸Œë¼ìš°ì € ë° í˜ì´ì§€ ìƒì„±
            browser = await playwright.chromium.launch(
                headless=False,
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--no-first-run',
                    '--disable-extensions',
                    '--disable-default-apps',
                    '--disable-gpu'
                ]
            )
            
            context = await browser.new_context(
                user_agent=user_agent,
                is_mobile=False,
                has_touch=False,
                viewport={'width': 1920, 'height': 1080}
            )
            
            page = await context.new_page()

            try:
                # ë¡œê·¸ì¸ ì²˜ë¦¬
                await login(page)

                # ë§í¬ ë°ì´í„° í™•ì¸ ë˜ëŠ” ìˆ˜ì§‘
                if os.path.exists('data/novelpia_novel_page_link_data.data'):
                    print("ğŸ“‚ ê¸°ì¡´ ë§í¬ ë°ì´í„° ë¡œë“œ ì¤‘...")
                    with open('data/novelpia_novel_page_link_data.data', 'rb') as f:
                        all_links = pickle.load(f)
                else:
                    print("ğŸ” ìƒˆë¡œìš´ ë§í¬ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                    
                    # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
                    last_page_num = await get_last_page(page, 'https://novelpia.com/plus/all/date/1/?main_genre=&is_please_write=')
                    print(f"ì´ í˜ì´ì§€ ìˆ˜: {last_page_num}")
                    
                    # URL ìƒì„± (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì²˜ìŒ 2í˜ì´ì§€ë§Œ)
                    urls = [f"https://novelpia.com/plus/all/date/{i}/?main_genre=&is_please_write=" for i in range(1, last_page_num)]  # í…ŒìŠ¤íŠ¸ìš©
                    
                    # ê° í˜ì´ì§€ì—ì„œ ìˆœì°¨ì ìœ¼ë¡œ ë§í¬ ìˆ˜ì§‘
                    all_links = []
                    for url in tqdm(urls, desc="ğŸ“„ í˜ì´ì§€ë³„ ë§í¬ ìˆ˜ì§‘"):
                        links = await get_links(page, url)
                        all_links.extend(links)
                        print(f"ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: {len(links)}")
                    
                    # ë§í¬ ë°ì´í„° ì €ì¥
                    with open('data/novelpia_novel_page_link_data.data', 'wb') as f:
                        pickle.dump(all_links, f)
                    print(f"ğŸ“ ì´ {len(all_links)}ê°œ ë§í¬ ì €ì¥ ì™„ë£Œ")

            except Exception as e:
                print(f"âŒ ë©”ì¸ ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {e}")
                import traceback
                traceback.print_exc()
            
            finally:
                await browser.close()

    # all_links = all_links[:10]
    all_links = split_data(all_links, 5)

    all_results = []
    async with async_playwright() as playwright:
        # ë§í¬ ìˆ˜ì§‘ ì§„í–‰ìƒí™© í‘œì‹œ
        for url_list in tqdm(all_links, desc="ğŸ“„ í˜ì´ì§€ë³„ ë§í¬ ìˆ˜ì§‘", unit="ë°°ì¹˜"):
            tasks = [get_data(playwright, url, ua.random) for url in url_list]
            
            # tqdm_asyncioë¡œ ê° ë°°ì¹˜ ë‚´ íƒœìŠ¤í¬ ì§„í–‰ìƒí™© í‘œì‹œ
            batch_results = await tqdm_asyncio.gather(
                *tasks, 
                desc=f"URL ì²˜ë¦¬ ({len(url_list)}ê°œ)", 
                unit="í˜ì´ì§€",
                # return_exceptions=True
            )
            all_results.extend(batch_results)

    # ë°ì´í„° ì €ì¥
    if all_results:
        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘... ({len(all_results)}ê°œ)")
        await save_data(all_results)
    else:
        print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    try:
        asyncio.run(main())
        print("\nğŸŠ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()